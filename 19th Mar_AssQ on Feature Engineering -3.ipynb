{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff55201-9462-4241-9c39-c74116fcc208",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625719c-5817-4c75-a75a-b3376dc0fb02",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a common data preprocessing technique used to rescale numeric features to a specific range. It transforms the data so that it falls within a predefined interval, typically between 0 and 1. This scaling method is useful when the features have different scales and ranges, and we want to bring them to a common scale to avoid dominance of certain features over others.\n",
    "\n",
    "The formula to perform Min-Max scaling on a feature x is as follows:\n",
    "\n",
    "x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "In this formula, min(x) represents the minimum value of the feature x, and max(x) represents the maximum value of the feature x.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Let's say we have a dataset of housing prices with two features: \"Area\" (measured in square feet) and \"Price\" (measured in dollars). The \"Area\" feature ranges from 500 to 3000 square feet, while the \"Price\" feature ranges from $50,000 to $500,000.\n",
    "\n",
    "Before applying Min-Max scaling, the data might look like this:\n",
    "\n",
    "Area | Price\n",
    "1500 | 200,000\n",
    "1000 | 150,000\n",
    "2500 | 300,000\n",
    "\n",
    "To scale the features using Min-Max scaling, we calculate the minimum and maximum values for each feature:\n",
    "\n",
    "min(Area) = 500\n",
    "max(Area) = 3000\n",
    "\n",
    "min(Price) = 50,000\n",
    "max(Price) = 500,000\n",
    "\n",
    "Next, we apply the Min-Max scaling formula to each value:\n",
    "\n",
    "Scaled_Area = (Area - min(Area)) / (max(Area) - min(Area))\n",
    "Scaled_Price = (Price - min(Price)) / (max(Price) - min(Price))\n",
    "\n",
    "After performing the calculations, the scaled data will look like this:\n",
    "\n",
    "Scaled_Area | Scaled_Price\n",
    "0.333 | 0.3\n",
    "0.000 | 0.0\n",
    "1.000 | 1.0\n",
    "\n",
    "As you can see, the \"Area\" values are now scaled between 0 and 1, and the \"Price\" values are also scaled between 0 and 1. This scaling ensures that both features have the same impact on any subsequent analysis or machine learning models that use the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a96be1-5ebb-4245-8b0c-4c959f02c66b",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1623135-9b8e-4a93-aed8-82a5d8c5449e",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization, is another data preprocessing method used for feature scaling. Unlike Min-Max scaling, which scales the features to a specific range, the Unit Vector technique scales the features to have a unit norm or length.\n",
    "\n",
    "In the Unit Vector technique, each feature vector is divided by its Euclidean norm, resulting in a vector with a length of 1. This normalization ensures that all the feature vectors have the same scale, but it does not preserve the original range of the data.\n",
    "\n",
    "The formula to perform Unit Vector scaling on a feature vector x is as follows:\n",
    "\n",
    "x_scaled = x / ||x||\n",
    "\n",
    "In this formula, ||x|| represents the Euclidean norm or length of the feature vector x.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Let's consider a dataset of customer reviews, where each review is represented by a feature vector indicating the frequency of occurrence of certain words. Suppose we have two features: \"love\" and \"hate,\" representing the number of times the words \"love\" and \"hate\" appear in each review.\n",
    "\n",
    "Before applying the Unit Vector technique, the data might look like this:\n",
    "\n",
    "love | hate\n",
    "2 | 1\n",
    "3 | 4\n",
    "1 | 2\n",
    "\n",
    "To scale the features using the Unit Vector technique, we calculate the Euclidean norm for each feature vector:\n",
    "\n",
    "||[2, 1]|| = sqrt(2^2 + 1^2) = sqrt(5) ≈ 2.236\n",
    "||[3, 4]|| = sqrt(3^2 + 4^2) = sqrt(25) = 5\n",
    "||[1, 2]|| = sqrt(1^2 + 2^2) = sqrt(5) ≈ 2.236\n",
    "\n",
    "Next, we divide each feature vector by its Euclidean norm:\n",
    "\n",
    "Scaled_love = [2, 1] / 2.236 ≈ [0.894, 0.447]\n",
    "Scaled_hate = [1, 2] / 2.236 ≈ [0.447, 0.894]\n",
    "\n",
    "After performing the calculations, the scaled data will look like this:\n",
    "\n",
    "Scaled_love | Scaled_hate\n",
    "0.894 | 0.447\n",
    "0.6 | 0.8\n",
    "0.447 | 0.894\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177d0d5-d256-4f7c-b71e-8324fee58db0",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26a828-48b5-428a-a871-19561e67b544",
   "metadata": {},
   "source": [
    "PCA, which stands for Principal Component Analysis, is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while preserving most of the important information. It achieves this by identifying the principal components, which are linear combinations of the original variables, capturing the maximum amount of variance in the data.\n",
    "\n",
    "The process of PCA involves the following steps:\n",
    "\n",
    "Standardize the data: If the features of the dataset have different scales, it is necessary to standardize them so that each feature has a mean of zero and a standard deviation of one. This step ensures that all variables contribute equally to the analysis.\n",
    "\n",
    "Compute the covariance matrix: The covariance matrix is calculated based on the standardized data, representing the relationships and variances between the different variables.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues are derived from the covariance matrix. The eigenvectors determine the directions of the principal components, and the eigenvalues represent the amount of variance explained by each component.\n",
    "\n",
    "Select the principal components: The eigenvectors are ranked according to their corresponding eigenvalues, with the highest eigenvalue indicating the most important component. By selecting a subset of these components, we can reduce the dimensionality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1e46c-ed58-42bb-b5cd-2b30bce43203",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eab84f-45bf-4a84-b29c-9a6fa824251c",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts, with PCA being a specific technique used for feature extraction. Feature extraction refers to the process of transforming the original set of features into a new set of features, typically with lower dimensionality, while preserving or maximizing the relevant information in the data.\n",
    "\n",
    "PCA can be used as a feature extraction method by identifying the principal components, which are linear combinations of the original features. These principal components are ordered based on their corresponding eigenvalues, indicating the amount of variance explained by each component. By selecting a subset of the principal components, we can effectively reduce the dimensionality of the data while retaining the most important information.\n",
    "\n",
    "Here's an example to illustrate the concept of using PCA for feature extraction:\n",
    "\n",
    "Suppose we have a dataset with 100 images, each represented by a vector of 1,000 pixels. Each pixel represents a feature in the dataset, resulting in a high-dimensional input space. We want to extract a smaller set of features that captures the most important information in the images.\n",
    "\n",
    "We can apply PCA to this dataset, where each image is considered as a data point. By performing PCA, we obtain the principal components, which are essentially patterns or directions in the pixel space that explain the maximum variance.\n",
    "\n",
    "Let's say after performing PCA, we find that the first principal component is strongly related to the overall brightness of the images, while the second principal component captures the variation in the orientation of edges. These principal components can be seen as new features that summarize the information contained in the original pixels.\n",
    "\n",
    "We can choose to retain only a subset of the principal components, such as the first 50 components, which would result in a lower-dimensional representation of the images. Each image is now represented by a vector of 50 values instead of the original 1,000 pixels.\n",
    "\n",
    "This reduced-dimensional feature representation can be used for various tasks such as image classification, clustering, or visualization. By extracting the most relevant features using PCA, we can simplify the data, remove noise or redundancy, and improve computational efficiency while still retaining the essential information needed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f929f-1b91-4931-8bc5-7e5cd1e5cd0d",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2859a7-4a25-4979-bed0-ee38e410f3a6",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling on certain features such as price, rating, and delivery time. Min-Max scaling, also known as normalization, is a technique used to transform the data to a specific range, typically between 0 and 1.\n",
    "\n",
    "Here's how you would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "Identify the features: In this case, the features are price, rating, and delivery time.\n",
    "\n",
    "Determine the minimum and maximum values: Calculate the minimum and maximum values for each feature in the dataset. For example, find the minimum and maximum prices, ratings, and delivery times in the dataset.\n",
    "\n",
    "Apply the Min-Max scaling formula: For each feature, use the following formula to scale the values:\n",
    "\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "This formula scales the original value to a range between 0 and 1 based on the minimum and maximum values of that feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9072e13-b584-48d9-992f-ab75126295f4",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a3b83-7d0e-4fc6-bf1c-715448e2b472",
   "metadata": {},
   "source": [
    "When building a model to predict stock prices with a dataset containing numerous features like company financial data and market trends, PCA can be used to reduce the dimensionality of the dataset. By reducing the number of features, PCA can help simplify the model and improve its computational efficiency, while still retaining most of the important information.\n",
    "\n",
    "Here's how you can use PCA for dimensionality reduction in the context of predicting stock prices:\n",
    "\n",
    "Data preprocessing: Start by preprocessing the dataset, which may involve cleaning the data, handling missing values, and normalizing the features to ensure they are on a similar scale. This step is crucial before applying PCA.\n",
    "\n",
    "Select the features: Identify the relevant features from the dataset that you want to include in the dimensionality reduction process. These features should be related to company financial data and market trends, such as stock price history, earnings per share, market indices, trading volumes, or other financial indicators.\n",
    "\n",
    "Apply PCA: Once you have selected the relevant features, apply PCA to the dataset. PCA will transform the original features into a set of linearly uncorrelated components called principal components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9131ea-5be3-4783-8e53-2b3d3889260f",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1049472-9a6c-46b3-8a76-44d49fa372d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "data = [1,5,10,15,20]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "reshaped_data = [[x] for x in data]\n",
    "scaled_data = scaler.fit_transform(reshaped_data)\n",
    "scaled_data = [x[0] for x in scaled_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0a53831-092b-4c21-b849-a1819629e202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9999999999999999,\n",
       " -0.5789473684210525,\n",
       " -0.05263157894736836,\n",
       " 0.47368421052631593,\n",
       " 1.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574cbea2-22f5-41ce-9a00-64df1e0acd83",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee018382-6584-41fc-9489-1e659cbbed61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components to retain: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = {\n",
    "    'height': [170, 165, 180, 160],\n",
    "    'weight': [65, 60, 75, 55],\n",
    "    'age': [30, 35, 45, 28],\n",
    "    'gender': [1, 0, 1, 0],\n",
    "    'blood pressure': [120, 110, 130, 115]\n",
    "}\n",
    "\n",
    "# Convert the data dictionary to a numpy array\n",
    "X = np.array(list(data.values())).T\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Determine the number of principal components to retain\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Find the number of components that explain at least 95% of the variance\n",
    "num_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "print(\"Number of principal components to retain:\", num_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c302964-45e2-421f-a08a-87bf2ff6facb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac591fea-4f0b-4e52-8cf4-82350907a582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
